# 第四次实践课作业-Transformer中英翻译
**罗浩铭 PB21030838**

## 题目
修改课程中的代码，使其完成中英翻译（原代码是英中翻译） 同时，将encoder layer的个数设为4，将decoder layer的个数设为5 展示训练后的中英翻译结果

## 实验内容
### 实现中英翻译


### 修改encoder与decoder的个数



### debug
#### DEVICE类型判断条件错误
源代码中存在如下的判断语句：
```python
if DEVICE == "cuda":
```

但由于`DEVICE = torch.device("cuda")`，因此`DEVICE`的类型是`torch.device`，而不是`str`，因此上述判断语句永远为`False`，与预期值`True`不符。

因此，需要将上述判断语句修改为：
```python
if DEVICE.__str__() == "cuda":
```

## 实验结果

|  实验组别  | best-loss |
| :--------: | :-------: |
|  baseline  |   1.788   |
| pretrained |   0.834   |


## 附：练习
#### 0. 看看原始数据长什么样？
略
#### 1. 把代码跑通（所有的图都是可以展示的，对于理解Transformer非常有用）(含Debug和非Debug)
略
#### 1.5 试试预训练模型 save/models/large_model.pt， 与你自己训的对比一下 
Baseline翻译测试结果如下：
<img src="./img/baseline-result.png" width="100%" style="margin: 0 auto;">
预训练模型翻译测试结果如下：
<img src="./img/pretrained-result.png" width="100%" style="margin: 0 auto;">

二者的翻译效果不相上下。

不过，Baseline的best-loss是1.788，预训练模型的best-loss是0.834，预训练模型的loss更低。

#### 2. 看一看建立的英文词典和中文词典长什么样，思考一下为什么要有 "UNK" "BOS" "EOS"
建立的英文词典和中文词典如下：
<img src="./img/en_cn_dict.png" width="100%" style="margin: 0 auto;">

其为python的`dict`对象，key为词，value为词的编号。其中，`UNK`表示未知词，编号为0；`PAD`表示填充词，编号为1；其余词为数据集分词后出现过的词，按出现频率从高到低编号为2~N。两个词典中，`BOS`表示句子的开始，编号均为2；`EOS`表示句子的结束，编号均为3。

`UNK`、`PAD`、`BOS`、`EOS`的作用如下：
- `UNK`：未知词，当输入不在训练集里时，模型会遇到未知词时，这时需要将其替换为`UNK`，以使得模型可以正常运行。
- `PAD`：填充词，句子长度小于模型接收的最大token长度时，用于填充剩余部分。
- `BOS`：句子开始，`BOS`用于提示模型开始翻译一个句子。
- `EOS`：句子结束，`EOS`用于提示模型结束翻译一个句子。

#### 3. word embedding后的词长什么样？
word embedding由`Embeddings`类实现，代码如下：
```python
class Embeddings(nn.Module):
    def __init__(self, d_model, vocab):
        super(Embeddings, self).__init__()
        self.lut = nn.Embedding(vocab, d_model)
        self.d_model = d_model

    def forward(self, x):
        # return x's embedding vector（times math.sqrt(d_model)）
        return self.lut(x) * math.sqrt(self.d_model)
```

它使用了一个`nn.Embedding`对象，类似于查找表，输入每个词的编号即可得到每个该词的embedding，其为一个`d_model = D_MODEL = 256`维的向量。

因此，word embedding后每个词都是一个`d_model = D_MODEL = 256`维的向量。

#### 4. 思考一下，为什么要加positional_embedding
这是为了让模型能够获得词的位置信息。因为Transformer架构中输入的各个位置是同时进行处理，在微结构上都是对等的，互相之间没有顺序关系，所以需要额外的方式来让模型学习到词的位置信息。
